---
editor_options: 
  chunk_output_type: console
---

# Работа с текстами: `gutenbergr`, `tidytext`, `stopwords`, `udpipe`

Привет, дорогие пацаны и пацанессы!

День второй, вы, предположительно, поразвлекались с регулярками, жизнь продолжается, давайте же анализировать тексты!

В этом эпизоде сериала АйсДан мы выясним, как обычно выглядят текстовые данные (и откуда их, кстати, брать!), как мы хотим, чтобы текстовые данные выглядели, и как перевести одно в другое. Бонус: смешные графики!

<iframe width="560" height="315" src="https://www.youtube.com/embed/Pi4_LNX4IyY" frameborder="0" allowfullscreen></iframe>

Для начала давайте установим нужные библиотеки:

```{r libs, message=FALSE, warning=FALSE, include=FALSE}
library(gutenbergr)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(ggplot2)
library(tm)
library(wesanderson) # https://github.com/karthik/wesanderson
library(udpipe)
library(DT)
library(ggwordcloud)
library(stringr)
library(igraph)
library(ggraph)
```

## Загрузка текста в R
В пакете `readr`, который входит в `tidyverse`, есть функция `read_lines()`. Такая же по сути функция есть в base R, она называется `readLines()`, но она работает гораздо медленнее, так что мы ей пользоваться не будем.

`read_lines()` принимает на вход путь к файлу с текстом на вашем компьютере или ссылку на файл в Интернете. Например, у Гарика на гитхабе есть документ с текстом книги Теда Чана "История твоей жизни" (судя по Википедии, это научно-фантастическая повесть о лингвистке, изучающей язык пришельцев. Звучит прикольно). Давайте считаем этот файл.

```{r}
t <- read_lines("https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/Chang.txt")
head(t)
```
`read_lines()` создал вектор строк:
```{r}
class(t)
```

В каждом элементе вектора у нас содержится одна строчка (в смысле, line) из книги. Чтобы превратить текст в единое целое, воспользуемся уже известной нам функцией `str_c()` из библиотеки `stringr`, и склеим, используя пробел как разделитель.

```{r}
t2 <- stringr::str_c(t, collapse = " ")
length(t2)
str_length(t2)
```

При таком слиянии стоит проверить, не было ли в анализируемом тексте знаков переноса, иначе они сольются неправильно:

```{r}
str_c(c("... она запо-", "лучила ..."), collapse = " ")
```


## `gutenbergr`
Библиотека `gutenbergr` это API для [проекта Gutenberg](http://www.gutenberg.org/) - онлайн-библиотеки электронных книг, которую создал Майкл Харт, изобретатель, собственно, формата электронных книг. Там хранится куча документов, которые по каким-то причинам не защищены авторским правом, так что мы можем совершенно легально их скачивать и анализировать (ну, или читать).

В этой библиотеке нас интересуют две вещи: объект `gutenberg_metadata`, в котором хранится информация о всех книгах, которые есть в библиотеке, и функция `gutenberg_download()`, которая позволяет их скачивать. Начнём с первого.

```{r}
str(gutenberg_metadata)
```

У каждого документа указан автор (если он есть) и название, `author` и `title`. Например, мы можем узнать, книг какого автора в библиотеке больше всего:

```{r}
gutenberg_metadata %>%
  count(author, sort = TRUE)
```

 Сколько произведений Джейн Остин (не перепутайте с другими Остин) есть в датасете?

```{r}
gutenberg_metadata %>% 
  filter(author == "Austen, Jane") %>% 
  distinct(gutenberg_id, title)
```

Ещё у каждой книги есть свой уникальный ID, который хранится в колонке `gutenberg_id`. По этому ID книгу можно скачать, используя функцию `gutenberg_download()`. Давайте скачаем "Эмму":

```{r download_emma, cache=TRUE}
emma <- gutenberg_download(158, mirror = "http://mirrors.xmission.com/gutenberg/")
emma
```

Можно скачивать сразу несколько книг. Давайте добавим еще "Леди Сьюзен":

```{r download_books, cache=TRUE}
books <- gutenberg_download(c(158, 946), meta_fields = "title", mirror = "http://mirrors.xmission.com/gutenberg/")
books
books %>% 
  count(title)
```

```{block, type = "rmdtask"}
Сколько уникальных заголовков из базы данных содержит "Sherlock Holmes"?
```

```{r, echo=FALSE, results='asis'}
library(checkdown)
gutenberg_metadata %>% 
  filter(str_detect(title, "Sherlock Holmes")) %>% 
  distinct(title) %>% 
  nrow() %>% 
  check_question()
```


## `tidytext` и `stopwords`
Сейчас наши книги хранятся в тиббле, в котором есть три колонки:
```{r}
class(books)
colnames(books)
```

Причём для каждой из книг у нас куча строк:
```{r}
books %>% count(title)
```

Это потому что одна строка в тиббле это одна строка книги. Мы можем снова воспользоваться функцией `str_c()` и слить весь текст в одну гигантскую строку, но вместо этого мы токенизируем наши тексты, используя в качестве токенов (=смысловых единиц) слова. Если вы посмотрели видео в начале, то уже знаете, что такое токенизация, а если не посмотрели, то идите и посмотрите :)

Для токенизации мы будем использовать функцию `unnest_tokens()` из библиотеки `tidytext` (про эту библиотеку есть книга, которую можно прочитать  [здесь](https://www.tidytextmining.com/)). В аргумент `output` функции `unnest_tokens()` подается вектор с именем будущей переменной, а аргумент `input` принимает имя переменной, в которой в нашем тиббле хранится текст. По умолчанию `unnest_tokens()` делит текст на слова, хотя есть и другие опции, которые можно указать в аргументе `token`. Но пока давайте поисследуем слова.

```{r}
library(tidytext)
books %>% 
  unnest_tokens(output = "word", input = text)
```

Теперь можно посчитать самые частотные слова в обоих произведениях:

```{r}
books %>% 
  unnest_tokens(output = "word", input = text) %>% 
  count(title, word, sort = TRUE)
```

Ну... Это было ожидаемо. Нужно убрать стоп-слова. Английские стоп-слова встроены в пакет `tidytext` (переменная `stop_words`):

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  anti_join(stop_words)
```

```{block, type = "rmdtask"}
Постройте следующий график, на котором представлены самые частотные 20 слов каждого из произведений.
```

```{r, echo = FALSE, message=FALSE}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>% 
  anti_join(stop_words) %>% 
  group_by(title) %>% 
  slice(1:20) %>% 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~title, scale = "free")
```

Как видно, на графике всё не упорядочено, давайте начнем с такого примера:

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(word, sort = TRUE) %>%
  anti_join(stop_words) %>% 
  slice(1:20) %>% 
  ggplot(aes(n, word))+
  geom_col()
```

Если мы работаем с одним фасетом, то все проблемы может решить функция `fct_reorder()`, которая упорядочивает на основании некоторой переменной:

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(word, sort = TRUE) %>% 
  anti_join(stop_words) %>% 
  slice(1:20) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(n, word))+
  geom_col()
```

Однако, если мы применим это к нашим данным, то получится неупорядочено, потому что `fct_reorder()`упорядочивает, не учитывая, где какой текст:

```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>%
  anti_join(stop_words) %>% 
  group_by(title) %>% 
  slice(1:20) %>% 
  ungroup() %>%
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~title, scales = "free")
```

В пакете `tidytext` есть функция `reorder_within()`, которая позволяет упорядочить нужным образом:
```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>%
  anti_join(stop_words) %>% 
  group_by(title) %>% 
  slice(1:20) %>% 
  ungroup() %>%
  mutate(word = reorder_within(x = word, by = n, within = title)) %>% 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~title, scales = "free")
```

Чтобы избавиться от дополнительной подписи, нужно использовать `scale_y_reordered()` или `scale_x_reordered()`:
```{r}
books %>% 
  unnest_tokens(word, text) %>% 
  count(title, word, sort = TRUE) %>%
  anti_join(stop_words) %>% 
  group_by(title) %>% 
  slice(1:20) %>% 
  ungroup() %>%
  mutate(word = reorder_within(x = word, by = n, within = title)) %>% 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~title, scales = "free")+
  scale_y_reordered()
```


Ещё один способ графически представить самые частотные слова это сделать облако слов. Есть библиотека `wordclouds` с функцией `wordcloud`. 

Давайте построим облако слов для романа Lady Susan. Для раскрашивания слов в разные цвета я использую палитру из великой библиотеки `wesanderson` с цветами разных фильмов, собственно, Уэса Андерсона. 

```{r}
pal <- wes_palette("Royal2")

books %>% filter(title == 'Lady Susan') %>% 
  unnest_tokens(input = 'text', output = 'word') %>%
  count(title, word, sort = TRUE) %>% anti_join(stop_words) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```

`wordcloud`, к сожалению, не совместим с `ggplot2` без которого, например, гораздо сложнее сделать фасетизацию и не задолбаться. Зато есть библиотека `ggwordcloud`, в которой есть `geom_text_wordcloud`. Воспользуемся же им!

Предварительное предупреждение: если слов в облако надо вместить много, то ``ggwordcloud` указывает для редко встречающихся очень маленький размер шрифта. `ggplot` от этого волнуется и выдаёт ворнинги, по одному на каждое слово. Это я к чему: график в чанке снизу выдаёт миллион ворнингов. В этом мануале они спрятаны, а когда столкнётесь с ними при выполнении заданий, можете их игнорировать. И, конечно, совершенно не обязательно включать слова, которые встречаются всего пару раз в очень длинном тексте.

```{r, memessage=FALSE, warning=FALSE}
books %>% 
  unnest_tokens(input = 'text', output = 'word') %>%
  count(title, word, sort = TRUE) %>% anti_join(stop_words) %>%
  filter(n > 20) %>% 
  ggplot(aes(label = word, size = n, color = n)) + geom_text_wordcloud(rm_outside = TRUE) + facet_wrap(~title, scale = 'free') + scale_size_area(max_size = 10)
```

Поиск самых частотных слов --- не единственная задача, которую можно решать при работе с текстом. Иногда имеет смысл узнать распределение слов в произведении. Давайте посмотрим как распределены в романе "Эмма" фамилии главных героев:

```{r}
books %>% 
  filter(title == "Emma") %>% 
  unnest_tokens(word, text) %>% 
  mutate(narrative_time = 1:n()) %>% 
  filter(str_detect(word, "knightley$|woodhouse$|churchill$|fairfax$")) %>%  
  ggplot()+
      geom_vline(aes(xintercept = narrative_time))+
  facet_wrap(~word, ncol = 1)
```

Функция `unnest_tokens()` позволяет работать не только со словами, но и, напрмиер, с n-граммами, то есть сочетаниями из n слов. Важно понимать, что n-граммы образуются "внахлёст":

```{r}
txt <- tibble(text = "I'm a Barbie girl in a Barbie world")

txt %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Чтобы преобразовать текст в биграммы, надо уточнить, что `token = "ngrams"`, а `n=2`.

```{r}
books %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Если мы посмотрим на самые частотные биграммы, то увидим кучу несодержательных вещей. 

```{r}
books %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
```

Почему? Снова стоп-слова-злодеи! Чтобы их убрать, надо разделить биграммы, отфильтровать так, чтобы остались только биграммы без стоп-слов, и снова склеить. Для этого мы воспользуемся функциями `separate` и `unite`, вот так (и заодно избавимся от NA'ев):

```{r}
books %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  drop_na()%>%
  separate(bigram, c('word1', 'word2'), sep = ' ') %>%
  filter(!(word1 %in% stop_words$word) 
          & !(word2 %in% stop_words$word)) %>% 
  count(word1, word2, sort = TRUE) %>% 
  unite(bigram, word1, word2, sep = " ")
```

## Визуализация биграмм с `igraph` и `ggraph`

Биграммы можно представить теми же способами, что и слова, а можно визуализировать сеть связей между словами в таком как бы графе. Для этого мы будем пользоваться двумя библиотеками: `igraph` и `ggraph`.

Из `igraph` нам понадобится функция `graph_from_data_frame()`, которой мы скормим тиббл с данными о частотности биграмм, а `ggraph` будем использовать, чтобы построить график.

Для графика нам нужно три переменных:
- from, от какого "узла" (слова) начинается связь
- to, в какой узел (слово) связь идёт
- weight, вес этой связи. В нашем случае - насколько часто эта связь (то есть, конкретная биграмма) встречается в тексте.

`graph_from_data_frame()` берёт тиббл (или любой дата фрейм) с этой информацией и преобразует его в объект, из которого можно делать граф.

Давайте совершим все эти преобразования и заодно оставим только биграммы, которых больше 20:

```{r}
bigrams_graph <- books %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  drop_na() %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
    filter(!(word1 %in% stop_words$word)
         & !(word2 %in% stop_words$word)) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 20) %>%
  graph_from_data_frame()

bigrams_graph
```


А теперь можно и график делать:

```{r}
ggraph(bigrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


## Пакет `stopwords`

Выше мы упомянули, что в пакет `tidytext` встроен список английских стоп-слов. Стоп-слова для других язков можно раздобыть, используя пакет `stopwords`. Вместо имени языка, функция принимает ISO-код языка:

```{r}
library(stopwords)
stopwords("ru")
```

Пакет предоставляет несколько источников списков:
```{r}
stopwords_getsources()
```

Давайте посмотрим, какие языки сейчас доступны:

```{r}
map(stopwords_getsources(), stopwords_getlanguages)
```

Мы видим, что есть несколько источников для русского языка:
```{r}
length(stopwords("ru", source = "snowball"))
length(stopwords("ru", source = "stopwords-iso"))
```

В зависимости от того, насколько консервативными вы хотите быть в плане стоп-слов (например, "сказал" это стоп-слово или нет?), можете выбирать тот или другой список. Ну и всегда можно попробовать оба и выбрать тот, который даёт более осмысленный результат.

## Пакет `udpipe`

Пакет `udpipe` представляет лемматизацию, морфологический и синтаксический анализ разных языков. Туториал можно найти [здесь](https://bnosac.github.io/udpipe/docs/doc1.html), там же есть список доступных языков.

```{r}
library(udpipe)
```

Модели качаются очень долго.
```{r download_en_model, cache=TRUE}
enmodel <- udpipe_download_model(language = "english")
```

Теперь можно распарсить какое-нибудь предложение:
```{r}
udpipe("The want of Miss Taylor would be felt every hour of every day.", object = enmodel)
```

Скачаем русскую модель:
```{r  download_ru_model, cache=TRUE}
rumodel <- udpipe_download_model(language = "russian-syntagrus")
```

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.", object = rumodel)
```

После того, как модель скачана, можно уже к ней обращаться просто по имени файла:

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.", object = rumodel)
```

`udpipe` лемматизирует наш текст (то есть, теперь "мясо" и "мяса" распрознаются как одно и то же слово), а также помечает, к какой части речи это слово относится, по [универсальной классификации](https://universaldependencies.org/u/pos/index.html). С таким текстом, например, можно посмотреть на то, как часто встречаются только определённые части речи.

И последний комментарий про `udpipe`: лемматизациz дело не быстрое, поэтому, скорее всего, лемматизировать все фанфики скопом у вас не получится. Вместо этого предлагается сделать сэмпл (то есть, рандомно выбрать, например, 300 фанфиков) и работать с ними. Если 300 вашему компьютеру тяжело - можно меньше. Соответственно, в заданиях, где нужна лемматизация, сэмплируйте датасет и работайте с сэмплом.

А теперь - собственно, задания!

## Задания

```{block, type = "rmdtask"}
Найдите три самыx популярных (по количеству лайков) фанфика и постройте барплоты для самых часто встречающихся слов в этих фанфиках.
```
```{r, eval = FALSE, echo = FALSE}
path <- "data_ficbook_death_note.csv"
dat <- read_csv(path)
ru_stopwords <- stopwords('ru')

three_most_liked <- dat %>% arrange(desc(likes)) %>% slice(1:3)

three_most_liked %>%
  unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>%
  separate(bigram, c('word1', 'word2'), sep = ' ') %>% 
  filter(!(word1 %in% ru_stopwords)) %>% 
  filter(!(word2 %in% ru_stopwords)) %>% 
  unite(bigram, word1, word2, sep = ' ') %>% 
  group_by(link, title) %>% 
  count(bigram, sort = TRUE) %>% 
  filter(n > 1) %>%
  ggplot(aes(x = n, y = bigram)) +geom_col() + facet_wrap(.~title, scale = 'free_y') + theme_classic()
```


```{block, type = "rmdtask"}
Найдите самый длинный фанфик (не забывайте, что в нашем датасете одна строка это одна глава, а глав бывает несколько) и постройте для него граф биграмм (не всех, конечно, а тех, что встречаются чаще скольки-то раз).
```

```{r, eval = FALSE, echo = FALSE}
longest_fic <- dat %>%
  group_by(link) %>%
  mutate(text = paste0(text, collapse = ''), doc_id = title) %>%
  slice(1) %>% ungroup() %>%
  mutate(length = str_length(text)) %>%
  arrange(desc(length)) %>% slice(1)

longest_fic %>% 
  unnest_tokens(word, text, token = 'words') %>%
  filter(!(word %in% ru_stopwords)) %>%
  count(word, sort = TRUE) %>%
  filter(n > 100)


longest_fic_graph_obj <- longest_fic %>% unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>%
  separate(bigram, c('word1', 'word2'), sep = ' ') %>% 
  filter(!(word1 %in% ru_stopwords)) %>% 
  filter(!(word2 %in% ru_stopwords)) %>% 
  count(word1, word2, sort = TRUE) %>%
  filter(n > 30) %>%
  graph_from_data_frame()

ggraph(longest_fic_graph_obj, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

```{block, type = "rmdtask"}
Для того же самого длинного фанфика по самым частотным словам поймите, какие в нём есть персонажи и как их зовут. Постройте график, который показывает, как частота появления разных персонажей меняется на протяжении фанфика.
```
```{r, eval = FALSE, echo = FALSE}
longest_fic %>% 
  unnest_tokens(word, text) %>% 
  mutate(narrative_time = 1:n()) %>% 
  filter(str_detect(word, "рюдзаки$|лайт$|мелло$|кира$|ниа$")) %>%  
  filter(word != 'лоулайт') %>% 
  ggplot()+
  geom_vline(aes(xintercept = narrative_time, color = word))+
  facet_wrap(~word, ncol = 1)
```

```{block, type = "rmdtask"}
Какие прилагательные чаще всего используются в вашем фандоме со словом "глаз" (в любой форме)? Проиллюстрируйте облаком слов. Именно здесь вам понадобится лемматизация, так что используйте сэмпл.
```
```{r, eval = FALSE, echo = FALSE}
rumodel <- udpipe_download_model(language = "russian-syntagrus") #этой строке нужно время


subsample_links <- unique(dat$link) %>% sample(30) 
subsample_dat <- dat %>% filter(link %in% subsample_links)
subsample_lemmatized <- subsample_dat %>% group_by(link) %>% mutate(text = paste0(text, collapse = ''), doc_id = title) %>% slice(1) %>% ungroup() %>% udpipe(object = rumodel) #и этой строке нужно время


subsample_lemmatized %>% filter(lead(lemma,1) == 'глаз' & upos == 'ADJ') %>% count(lemma, sort = TRUE) %>% slice(1:30) %>% ggplot(aes(label = lemma, size = n, color = n)) + geom_text_wordcloud(rm_outside = TRUE)  + scale_size_area(max_size = 30)
```

```{block, type = "rmdtask"}
Найдите самого плодовитого автора в вашем фандоме (то есть, такого, который написал больше всего фанфиков). Попробуйте найти клише, которые встречаются в его или её текстах. Тут можно посмотреть на биграммы, триграммы или и то, и другое - посмотрите, что интереснее, и покажите на графике (можно барплот, можно сделать облако слов, можете придумать свой вариант).
```
```{r, eval = FALSE, echo = FALSE}
# Некоторые мысли о вещах, с которыми тут можно поиграть:
# 1) взять триграммы и не так строго фильтровать стоп-слова (в смысле, использовать более короткий список из библиотеки stopwords, или фильтровать с ИЛИ (слово1 не стоп-слово ИЛИ слово2 не стоп-слово))
# 2) лемматизировать и поубирать биграммы/триграммы с какими-нибудь частями речи
# 3) добавлять в список доп. стоп-слова, специфические для автора

most_author <- dat %>% group_by(author) %>% summarize(n = n_distinct(link)) %>% arrange(desc(n)) %>% slice(1) %>% select(author)

dat %>%
  filter(author == most_author$author) %>% 
  unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>%
  separate(bigram, c('word1', 'word2'), sep = ' ') %>% 
  filter(!(word1 %in% ru_stopwords)) %>%
  filter(!(word2 %in% ru_stopwords)) %>% 
  unite(bigram, word1, word2, sep = ' ') %>%
  count(bigram, sort = TRUE) %>%
  filter(n > 20) %>%
  ggplot(aes(label = bigram, size = n, color = n)) + geom_text_wordcloud(rm_outside = TRUE)  + scale_size_area(max_size = 30)
```

Если у вас появятся вопросы - смело задавайте их в канале #text-preprocessing-questions, а все странные и нелепые графики присылайте в #accidental-art. Удачи!
