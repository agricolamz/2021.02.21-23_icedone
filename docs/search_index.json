[["работа-с-текстами-gutenbergr-tidytext-stopwords-udpipe.html", "5 Работа с текстами: gutenbergr, tidytext, stopwords, udpipe 5.1 Загрузка текста в R 5.2 gutenbergr 5.3 tidytext и stopwords 5.4 Визуализация биграмм с igraph и ggraph 5.5 Пакет stopwords 5.6 Пакет udpipe 5.7 Задания", " 5 Работа с текстами: gutenbergr, tidytext, stopwords, udpipe Привет, дорогие пацаны и пацанессы! День второй, вы, предположительно, поразвлекались с регулярками, жизнь продолжается, давайте же анализировать тексты! В этом эпизоде сериала АйсДан мы выясним, как обычно выглядят текстовые данные (и откуда их, кстати, брать!), как мы хотим, чтобы текстовые данные выглядели, и как перевести одно в другое. Бонус: смешные графики! МЕСТО ДЛЯ ВИДЕО :)) В видео: 1) форматы текстовых данных (массив текста -&gt; term-document matrix) 2) Токены и токенизация 3) Стемминг и лемматизация Для начала давайте установим нужные библиотеки: 5.1 Загрузка текста в R В пакете readr, который входит в tidyverse, есть функция read_lines(). Такая же по сути функция есть в base R, она называется readLines(), но она работает гораздо медленнее, так что мы ей пользоваться не будем. read_lines() принимает на вход путь к файлу с текстом на вашем компьютере или ссылку на файл в Интернете. Например, у Гарика на гитхабе есть документ с текстом книги Теда Чана “История твоей жизни” (судя по Википедии, это научно-фантастическая повесть о лингвистке, изучающей язык пришельцев. Звучит прикольно). Давайте считаем этот файл. t &lt;- read_lines(&quot;https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/Chang.txt&quot;) head(t) ## [1] &quot;Тед Чан&quot; ## [2] &quot;История твоей жизни&quot; ## [3] &quot;Твой отец собирается задать мне вопрос. Это самый важный момент в нашей жизни, и я хочу&quot; ## [4] &quot;запомнить все до малейшей детали. Уже за полночь, но мы только что вернулись домой после&quot; ## [5] &quot;ужина в ресторане и веселого шоу и сразу выходим в патио полюбоваться полной луной. Хочу&quot; ## [6] &quot;танцевать! — объявляю я, и твой отец подтрунивает надо мной, но мы начинаем скользить в&quot; read_lines() создал вектор строк: class(t) ## [1] &quot;character&quot; В каждом элементе вектора у нас содержится одна строчка (в смысле, line) из книги. Чтобы превратить текст в единое целое, воспользуемся уже известной нам функцией str_c() из библиотеки stringr, и склеим, используя пробел как разделитель. t2 &lt;- stringr::str_c(t, collapse = &quot; &quot;) length(t2) ## [1] 1 str_length(t2) ## [1] 117398 При таком слиянии стоит проверить, не было ли в анализируемом тексте знаков переноса, иначе они сольются неправильно: str_c(c(&quot;... она запо-&quot;, &quot;лучила ...&quot;), collapse = &quot; &quot;) ## [1] &quot;... она запо- лучила ...&quot; 5.2 gutenbergr Библиотека gutenbergr это API для проекта Gutenberg - онлайн-библиотеки электронных книг, которую создал Майкл Харт, изобретатель, собственно, формата электронных книг. Там хранится куча документов, которые по каким-то причинам не защищены авторским правом, так что мы можем совершенно легально их скачивать и анализировать (ну, или читать). В этой библиотеке нас интересуют две вещи: объект gutenberg_metadata, в котором хранится информация о всех книгах, которые есть в библиотеке, и функция gutenberg_download(), которая позволяет их скачивать. Начнём с первого. str(gutenberg_metadata) ## tibble [51,997 × 8] (S3: tbl_df/tbl/data.frame) ## $ gutenberg_id : int [1:51997] 0 1 2 3 4 5 6 7 8 9 ... ## $ title : chr [1:51997] NA &quot;The Declaration of Independence of the United States of America&quot; &quot;The United States Bill of Rights\\r\\nThe Ten Original Amendments to the Constitution of the United States&quot; &quot;John F. Kennedy&#39;s Inaugural Address&quot; ... ## $ author : chr [1:51997] NA &quot;Jefferson, Thomas&quot; &quot;United States&quot; &quot;Kennedy, John F. (John Fitzgerald)&quot; ... ## $ gutenberg_author_id: int [1:51997] NA 1638 1 1666 3 1 4 NA 3 3 ... ## $ language : chr [1:51997] &quot;en&quot; &quot;en&quot; &quot;en&quot; &quot;en&quot; ... ## $ gutenberg_bookshelf: chr [1:51997] NA &quot;United States Law/American Revolutionary War/Politics&quot; &quot;American Revolutionary War/Politics/United States Law&quot; NA ... ## $ rights : chr [1:51997] &quot;Public domain in the USA.&quot; &quot;Public domain in the USA.&quot; &quot;Public domain in the USA.&quot; &quot;Public domain in the USA.&quot; ... ## $ has_text : logi [1:51997] TRUE TRUE TRUE TRUE TRUE TRUE ... ## - attr(*, &quot;date_updated&quot;)= Date[1:1], format: &quot;2016-05-05&quot; У каждого документа указан автор (если он есть) и название, author и title. Например, мы можем узнать, книг какого автора в библиотеке больше всего: gutenberg_metadata %&gt;% count(author, sort = TRUE) Сколько произведений Джейн Остин (не перепутайте с другими Остин) есть в датасете? gutenberg_metadata %&gt;% filter(author == &quot;Austen, Jane&quot;) %&gt;% distinct(gutenberg_id, title) Ещё у каждой книги есть свой уникальный ID, который хранится в колонке gutenberg_id. По этому ID книгу можно скачать, используя функцию gutenberg_download(). Давайте скачаем “Эмму”: emma &lt;- gutenberg_download(158, mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) emma Можно скачивать сразу несколько книг. Давайте добавим еще “Леди Сьюзен”: books &lt;- gutenberg_download(c(158, 946), meta_fields = &quot;title&quot;, mirror = &quot;http://mirrors.xmission.com/gutenberg/&quot;) books books %&gt;% count(title) Сколько уникальных заголовков из базы данных содержит “Sherlock Holmes?” 5.3 tidytext и stopwords Сейчас наши книги хранятся в тиббле, в котором есть три колонки: class(books) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; colnames(books) ## [1] &quot;gutenberg_id&quot; &quot;text&quot; &quot;title&quot; Причём для каждой из книг у нас куча строк: books %&gt;% count(title) Это потому что одна строка в тиббле это одна строка книги. Мы можем снова воспользоваться функцией str_c() и слить весь текст в одну гигантскую строку, но вместо этого мы токенизируем наши тексты, используя в качестве токенов (=смысловых единиц) слова. Если вы посмотрели видео в начале, то уже знаете, что такое токенизация, а если не посмотрели, то идите и посмотрите :) Для токенизации мы будем использовать функцию unnest_tokens() из библиотеки tidytext (про эту библиотеку есть книга, которую можно прочитать здесь). В аргумент output функции unnest_tokens() подается вектор с именем будущей переменной, а аргумент input принимает имя переменной, в которой в нашем тиббле хранится текст. По умолчанию unnest_tokens() делит текст на слова, хотя есть и другие опции, которые можно указать в аргументе token. Но пока давайте поисследуем слова. library(tidytext) books %&gt;% unnest_tokens(output = &quot;word&quot;, input = text) Теперь можно посчитать самые частотные слова в обоих произведениях: books %&gt;% unnest_tokens(output = &quot;word&quot;, input = text) %&gt;% count(title, word, sort = TRUE) Ну… Это было ожидаемо. Нужно убрать стоп-слова. Английские стоп-слова встроены в пакет tidytext (переменная stop_words): books %&gt;% unnest_tokens(word, text) %&gt;% count(title, word, sort = TRUE) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; Постройте следующий график, на котором представлены самые частотные 20 слов каждого из произведений. Как видно, на графике всё не упорядочено, давайте начнем с такого примера: books %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% slice(1:20) %&gt;% ggplot(aes(n, word))+ geom_col() Если мы работаем с одним фасетом, то все проблемы может решить функция fct_reorder(), которая упорядочивает на основании некоторой переменной: books %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = TRUE) %&gt;% slice(1:20) %&gt;% mutate(word = fct_reorder(word, n)) %&gt;% ggplot(aes(n, word))+ geom_col() Однако, если мы применим это к нашим данным, то получится неупорядочено, потому что fct_reorder()упорядочивает, не учитывая, где какой текст: books %&gt;% unnest_tokens(word, text) %&gt;% count(title, word, sort = TRUE) %&gt;% group_by(title) %&gt;% slice(1:20) %&gt;% ungroup() %&gt;% mutate(word = fct_reorder(word, n)) %&gt;% ggplot(aes(n, word))+ geom_col()+ facet_wrap(~title, scales = &quot;free&quot;) В пакете tidytext есть функция reorder_within(), которая позволяет упорядочить нужным образом: books %&gt;% unnest_tokens(word, text) %&gt;% count(title, word, sort = TRUE) %&gt;% group_by(title) %&gt;% slice(1:20) %&gt;% ungroup() %&gt;% mutate(word = reorder_within(x = word, by = n, within = title)) %&gt;% ggplot(aes(n, word))+ geom_col()+ facet_wrap(~title, scales = &quot;free&quot;) Чтобы избавиться от дополнительной подписи, нужно использовать scale_y_reordered() или scale_x_reordered(): books %&gt;% unnest_tokens(word, text) %&gt;% count(title, word, sort = TRUE) %&gt;% group_by(title) %&gt;% slice(1:20) %&gt;% ungroup() %&gt;% mutate(word = reorder_within(x = word, by = n, within = title)) %&gt;% ggplot(aes(n, word))+ geom_col()+ facet_wrap(~title, scales = &quot;free&quot;)+ scale_y_reordered() Ещё один способ графически представить самые частотные слова это сделать облако слов. Есть библиотека wordclouds с функцией wordcloud. Давайте построим облако слов для романа Lady Susan. Для раскрашивания слов в разные цвета я использую палитру из великой библиотеки wesanderson с цветами разных фильмов, собственно, Уэса Андерсона. pal &lt;- wes_palette(&quot;Royal2&quot;) books %&gt;% filter(title == &#39;Lady Susan&#39;) %&gt;% unnest_tokens(input = &#39;text&#39;, output = &#39;word&#39;) %&gt;% count(title, word, sort = TRUE) %&gt;% anti_join(stop_words) %&gt;% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal)) ## Joining, by = &quot;word&quot; wordcloud, к сожалению, не совместим с ggplot2 без которого, например, гораздо сложнее сделать фасетизацию и не задолбаться. Зато есть библиотека ggwordcloud, в которой есть geom_text_wordcloud. Воспользуемся же им! Предварительное предупреждение: если слов в облако надо вместить много, то `ggwordcloud указывает для редко встречающихся очень маленький размер шрифта. ggplot от этого волнуется и выдаёт ворнинги, по одному на каждое слово. Это я к чему: график в чанке снизу выдаёт миллион ворнингов. В этом мануале они спрятаны, а когда столкнётесь с ними при выполнении заданий, можете их игнорировать. И, конечно, совершенно не обязательно включать слова, которые встречаются всего пару раз в очень длинном тексте. books %&gt;% unnest_tokens(input = &#39;text&#39;, output = &#39;word&#39;) %&gt;% count(title, word, sort = TRUE) %&gt;% anti_join(stop_words) %&gt;% filter(n &gt; 20) %&gt;% ggplot(aes(label = word, size = n, color = n)) + geom_text_wordcloud(rm_outside = TRUE) + facet_wrap(~title, scale = &#39;free&#39;) + scale_size_area(max_size = 10) ## Joining, by = &quot;word&quot; ## Some words could not fit on page. They have been removed. Поиск самых частотных слов — не единственная задача, которую можно решать при работе с текстом. Иногда имеет смысл узнать распределение слов в произведении. Давайте посмотрим как распределены в романе “Эмма” фамилии главных героев: books %&gt;% filter(title == &quot;Emma&quot;) %&gt;% unnest_tokens(word, text) %&gt;% mutate(narrative_time = 1:n()) %&gt;% filter(str_detect(word, &quot;knightley$|woodhouse$|churchill$|fairfax$&quot;)) %&gt;% ggplot()+ geom_vline(aes(xintercept = narrative_time))+ facet_wrap(~word, ncol = 1) Функция unnest_tokens() позволяет работать не только со словами, но и, напрмиер, с n-граммами, то есть сочетаниями из n слов. Важно понимать, что n-граммы образуются “внахлёст”: txt &lt;- tibble(text = &quot;I&#39;m a Barbie girl in a Barbie world&quot;) txt %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) Чтобы преобразовать текст в биграммы, надо уточнить, что token = \"ngrams\", а n=2. books %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) Если мы посмотрим на самые частотные биграммы, то увидим кучу несодержательных вещей. books %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% count(bigram, sort = TRUE) Почему? Снова стоп-слова-злодеи! Чтобы их убрать, надо разделить биграммы, отфильтровать так, чтобы остались только биграммы без стоп-слов, и снова склеить. Для этого мы воспользуемся функциями separate и unite, вот так (и заодно избавимся от NA’ев): books %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% drop_na()%&gt;% separate(bigram, c(&#39;word1&#39;, &#39;word2&#39;), sep = &#39; &#39;) %&gt;% filter(!(word1 %in% stop_words$word) &amp; !(word2 %in% stop_words$word)) %&gt;% count(word1, word2, sort = TRUE) %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) 5.4 Визуализация биграмм с igraph и ggraph Биграммы можно представить теми же способами, что и слова, а можно визуализировать сеть связей между словами в таком как бы графе. Для этого мы будем пользоваться двумя библиотеками: igraph и ggraph. Из igraph нам понадобится функция graph_from_data_frame(), которой мы скормим тиббл с данными о частотности биграмм, а ggraph будем использовать, чтобы построить график. Для графика нам нужно три переменных: - from, от какого “узла” (слова) начинается связь - to, в какой узел (слово) связь идёт - weight, вес этой связи. В нашем случае - насколько часто эта связь (то есть, конкретная биграмма) встречается в тексте. graph_from_data_frame() берёт тиббл (или любой дата фрейм) с этой информацией и преобразует его в объект, из которого можно делать граф. Давайте совершим все эти преобразования и заодно оставим только биграммы, которых больше 20: bigrams_graph &lt;- books %&gt;% unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2) %&gt;% drop_na() %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!(word1 %in% stop_words$word) &amp; !(word2 %in% stop_words$word)) %&gt;% count(word1, word2, sort = TRUE) %&gt;% filter(n &gt; 20) %&gt;% graph_from_data_frame() bigrams_graph ## IGRAPH bc51299 DN-- 25 17 -- ## + attr: name (v/c), n (e/n) ## + edges from bc51299 (vertex names): ## [1] miss -&gt;woodhouse frank -&gt;churchill miss -&gt;fairfax miss -&gt;bates ## [5] jane -&gt;fairfax lady -&gt;susan de -&gt;courcy miss -&gt;smith ## [9] sir -&gt;james john -&gt;knightley miss -&gt;taylor dear -&gt;emma ## [13] maple -&gt;grove cried -&gt;emma dear -&gt;miss harriet-&gt;smith ## [17] robert -&gt;martin А теперь можно и график делать: ggraph(bigrams_graph, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) 5.5 Пакет stopwords Выше мы упомянули, что в пакет tidytext встроен список английских стоп-слов. Стоп-слова для других язков можно раздобыть, используя пакет stopwords. Вместо имени языка, функция принимает ISO-код языка: library(stopwords) ## ## Attaching package: &#39;stopwords&#39; ## The following object is masked from &#39;package:tm&#39;: ## ## stopwords stopwords(&quot;ru&quot;) ## [1] &quot;и&quot; &quot;в&quot; &quot;во&quot; &quot;не&quot; &quot;что&quot; &quot;он&quot; &quot;на&quot; ## [8] &quot;я&quot; &quot;с&quot; &quot;со&quot; &quot;как&quot; &quot;а&quot; &quot;то&quot; &quot;все&quot; ## [15] &quot;она&quot; &quot;так&quot; &quot;его&quot; &quot;но&quot; &quot;да&quot; &quot;ты&quot; &quot;к&quot; ## [22] &quot;у&quot; &quot;же&quot; &quot;вы&quot; &quot;за&quot; &quot;бы&quot; &quot;по&quot; &quot;только&quot; ## [29] &quot;ее&quot; &quot;мне&quot; &quot;было&quot; &quot;вот&quot; &quot;от&quot; &quot;меня&quot; &quot;еще&quot; ## [36] &quot;нет&quot; &quot;о&quot; &quot;из&quot; &quot;ему&quot; &quot;теперь&quot; &quot;когда&quot; &quot;даже&quot; ## [43] &quot;ну&quot; &quot;вдруг&quot; &quot;ли&quot; &quot;если&quot; &quot;уже&quot; &quot;или&quot; &quot;ни&quot; ## [50] &quot;быть&quot; &quot;был&quot; &quot;него&quot; &quot;до&quot; &quot;вас&quot; &quot;нибудь&quot; &quot;опять&quot; ## [57] &quot;уж&quot; &quot;вам&quot; &quot;сказал&quot; &quot;ведь&quot; &quot;там&quot; &quot;потом&quot; &quot;себя&quot; ## [64] &quot;ничего&quot; &quot;ей&quot; &quot;может&quot; &quot;они&quot; &quot;тут&quot; &quot;где&quot; &quot;есть&quot; ## [71] &quot;надо&quot; &quot;ней&quot; &quot;для&quot; &quot;мы&quot; &quot;тебя&quot; &quot;их&quot; &quot;чем&quot; ## [78] &quot;была&quot; &quot;сам&quot; &quot;чтоб&quot; &quot;без&quot; &quot;будто&quot; &quot;человек&quot; &quot;чего&quot; ## [85] &quot;раз&quot; &quot;тоже&quot; &quot;себе&quot; &quot;под&quot; &quot;жизнь&quot; &quot;будет&quot; &quot;ж&quot; ## [92] &quot;тогда&quot; &quot;кто&quot; &quot;этот&quot; &quot;говорил&quot; &quot;того&quot; &quot;потому&quot; &quot;этого&quot; ## [99] &quot;какой&quot; &quot;совсем&quot; &quot;ним&quot; &quot;здесь&quot; &quot;этом&quot; &quot;один&quot; &quot;почти&quot; ## [106] &quot;мой&quot; &quot;тем&quot; &quot;чтобы&quot; &quot;нее&quot; &quot;кажется&quot; &quot;сейчас&quot; &quot;были&quot; ## [113] &quot;куда&quot; &quot;зачем&quot; &quot;сказать&quot; &quot;всех&quot; &quot;никогда&quot; &quot;сегодня&quot; &quot;можно&quot; ## [120] &quot;при&quot; &quot;наконец&quot; &quot;два&quot; &quot;об&quot; &quot;другой&quot; &quot;хоть&quot; &quot;после&quot; ## [127] &quot;над&quot; &quot;больше&quot; &quot;тот&quot; &quot;через&quot; &quot;эти&quot; &quot;нас&quot; &quot;про&quot; ## [134] &quot;всего&quot; &quot;них&quot; &quot;какая&quot; &quot;много&quot; &quot;разве&quot; &quot;сказала&quot; &quot;три&quot; ## [141] &quot;эту&quot; &quot;моя&quot; &quot;впрочем&quot; &quot;хорошо&quot; &quot;свою&quot; &quot;этой&quot; &quot;перед&quot; ## [148] &quot;иногда&quot; &quot;лучше&quot; &quot;чуть&quot; &quot;том&quot; &quot;нельзя&quot; &quot;такой&quot; &quot;им&quot; ## [155] &quot;более&quot; &quot;всегда&quot; &quot;конечно&quot; &quot;всю&quot; &quot;между&quot; Пакет предоставляет несколько источников списков: stopwords_getsources() ## [1] &quot;snowball&quot; &quot;stopwords-iso&quot; &quot;misc&quot; &quot;smart&quot; ## [5] &quot;marimo&quot; &quot;ancient&quot; &quot;nltk&quot; Давайте посмотрим, какие языки сейчас доступны: map(stopwords_getsources(), stopwords_getlanguages) ## [[1]] ## [1] &quot;da&quot; &quot;de&quot; &quot;en&quot; &quot;es&quot; &quot;fi&quot; &quot;fr&quot; &quot;hu&quot; &quot;ir&quot; &quot;it&quot; &quot;nl&quot; &quot;no&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sv&quot; ## ## [[2]] ## [1] &quot;af&quot; &quot;ar&quot; &quot;hy&quot; &quot;eu&quot; &quot;bn&quot; &quot;br&quot; &quot;bg&quot; &quot;ca&quot; &quot;zh&quot; &quot;hr&quot; &quot;cs&quot; &quot;da&quot; &quot;nl&quot; &quot;en&quot; &quot;eo&quot; ## [16] &quot;et&quot; &quot;fi&quot; &quot;fr&quot; &quot;gl&quot; &quot;de&quot; &quot;el&quot; &quot;ha&quot; &quot;he&quot; &quot;hi&quot; &quot;hu&quot; &quot;id&quot; &quot;ga&quot; &quot;it&quot; &quot;ja&quot; &quot;ko&quot; ## [31] &quot;ku&quot; &quot;la&quot; &quot;lt&quot; &quot;lv&quot; &quot;ms&quot; &quot;mr&quot; &quot;no&quot; &quot;fa&quot; &quot;pl&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sk&quot; &quot;sl&quot; &quot;so&quot; ## [46] &quot;st&quot; &quot;es&quot; &quot;sw&quot; &quot;sv&quot; &quot;th&quot; &quot;tl&quot; &quot;tr&quot; &quot;uk&quot; &quot;ur&quot; &quot;vi&quot; &quot;yo&quot; &quot;zu&quot; ## ## [[3]] ## [1] &quot;ar&quot; &quot;ca&quot; &quot;el&quot; &quot;gu&quot; &quot;zh&quot; ## ## [[4]] ## [1] &quot;en&quot; ## ## [[5]] ## [1] &quot;en&quot; &quot;ja&quot; &quot;ar&quot; &quot;he&quot; &quot;zh_tw&quot; &quot;zh_cn&quot; ## ## [[6]] ## [1] &quot;grc&quot; &quot;la&quot; ## ## [[7]] ## [1] &quot;ar&quot; &quot;az&quot; &quot;da&quot; &quot;nl&quot; &quot;en&quot; &quot;fi&quot; &quot;fr&quot; &quot;de&quot; &quot;el&quot; &quot;hu&quot; &quot;id&quot; &quot;it&quot; &quot;kk&quot; &quot;ne&quot; &quot;no&quot; ## [16] &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sl&quot; &quot;es&quot; &quot;sv&quot; &quot;tg&quot; &quot;tr&quot; Мы видим, что есть несколько источников для русского языка: length(stopwords(&quot;ru&quot;, source = &quot;snowball&quot;)) ## [1] 159 length(stopwords(&quot;ru&quot;, source = &quot;stopwords-iso&quot;)) ## [1] 559 В зависимости от того, насколько консервативными вы хотите быть в плане стоп-слов (например, “сказал” это стоп-слово или нет?), можете выбирать тот или другой список. Ну и всегда можно попробовать оба и выбрать тот, который даёт более осмысленный результат. 5.6 Пакет udpipe Пакет udpipe представляет лемматизацию, морфологический и синтаксический анализ разных языков. Туториал можно найти здесь, там же есть список доступных языков. library(udpipe) Модели качаются очень долго. enmodel &lt;- udpipe_download_model(language = &quot;english&quot;) ## Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /home/agricolamz/work/materials/2021.02.21-23_icedone/english-ewt-ud-2.5-191206.udpipe ## - This model has been trained on version 2.5 of data from https://universaldependencies.org ## - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0 ## - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details. ## - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette(&#39;udpipe-train&#39;, package = &#39;udpipe&#39;) ## Downloading finished, model stored at &#39;/home/agricolamz/work/materials/2021.02.21-23_icedone/english-ewt-ud-2.5-191206.udpipe&#39; Теперь можно распарсить какое-нибудь предложение: udpipe(&quot;The want of Miss Taylor would be felt every hour of every day.&quot;, object = enmodel) Скачаем русскую модель: rumodel &lt;- udpipe_download_model(language = &quot;russian-syntagrus&quot;) ## Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/russian-syntagrus-ud-2.5-191206.udpipe to /home/agricolamz/work/materials/2021.02.21-23_icedone/russian-syntagrus-ud-2.5-191206.udpipe ## - This model has been trained on version 2.5 of data from https://universaldependencies.org ## - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0 ## - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details. ## - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette(&#39;udpipe-train&#39;, package = &#39;udpipe&#39;) ## Downloading finished, model stored at &#39;/home/agricolamz/work/materials/2021.02.21-23_icedone/russian-syntagrus-ud-2.5-191206.udpipe&#39; udpipe(&quot;Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.&quot;, object = rumodel) После того, как модель скачана, можно уже к ней обращаться просто по имени файла: udpipe(&quot;Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.&quot;, object = rumodel) udpipe лемматизирует наш текст (то есть, теперь “мясо” и “мяса” распрознаются как одно и то же слово), а также помечает, к какой части речи это слово относится, по универсальной классификации. С таким текстом, например, можно посмотреть на то, как часто встречаются только определённые части речи. И последний комментарий про udpipe: лемматизациz дело не быстрое, поэтому, скорее всего, лемматизировать все фанфики скопом у вас не получится. Вместо этого предлагается сделать сэмпл (то есть, рандомно выбрать, например, 300 фанфиков) и работать с ними. Если 300 вашему компьютеру тяжело - можно меньше. Соответственно, в заданиях, где нужна лемматизация, сэмплируйте датасет и работайте с сэмплом. А теперь - собственно, задания! 5.7 Задания Найдите три самыx популярных (по количеству лайков) фанфика и постройте барплоты для самых часто встречающихся слов в этих фанфиках. Найдите самый длинный фанфик (не забывайте, что в нашем датасете одна строка это одна глава, а глав бывает несколько) и постройте для него граф биграмм (не всех, конечно, а тех, что встречаются чаще скольки-то раз). Для того же самого длинного фанфика по самым частотным словам поймите, какие в нём есть персонажи и как их зовут. Постройте график, который показывает, как частота появления разных персонажей меняется на протяжении фанфика. Какие прилагательные чаще всего используются в вашем фандоме со словом “глаз” (в любой форме)? Проиллюстрируйте облаком слов. Именно здесь вам понадобится лемматизация, так что используйте сэмпл. Найдите самого плодовитого автора в вашем фандоме (то есть, такого, который написал больше всего фанфиков). Попробуйте найти клише, которые встречаются в его или её текстах. Тут можно посмотреть на биграммы, триграммы или и то, и другое - посмотрите, что интереснее, и покажите на графике (можно барплот, можно сделать облако слов, можете придумать свой вариант). Если у вас появятся вопросы - смело задавайте их в канале #text-preprocessing-questions, а все странные и нелепые графики присылайте в #accidental-art. Удачи! "]]
