[["анализ-текста.html", "6 Анализ текста 6.1 Данные 6.2 tf-idf 6.3 Предиктивный ввод текста 6.4 Анализ тональности 6.5 Тематическое моделирование", " 6 Анализ текста 6.1 Данные Для работы мы воспользуемся двумя датасетами: Рассказы М. Зощенко zo &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/zoshenko.csv&quot;) zo Курс начертательной геометрии под редакцией В.Гордона geom &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/gordon_geometry.csv&quot;) Для начала лемматизируем полуичвшиеся тексты: library(udpipe) rus &lt;- udpipe_load_model(&quot;russian-syntagrus-ud-2.4-190531.udpipe&quot;) geom_tokenized &lt;- udpipe(geom, object = rus) zo_tokenized &lt;- udpipe(zo, object = rus) Уберем стопслова и леммы, содержащие цифры и знаки препинания library(stopwords) sw &lt;- tibble(lemma = stopwords(language = &quot;ru&quot;)) geom_tokenized %&gt;% bind_rows(zo_tokenized) %&gt;% filter(!str_detect(lemma, &quot;\\\\W|\\\\d&quot;)) %&gt;% anti_join(sw) %&gt;% select(doc_id, sentence_id, lemma) -&gt; all_texts all_texts Используйте библиотеку gutenbergr и скачайте “Чувство и чувствительность”(Sense and Sensibility, gutenberg_id = 161) и “Гордость и предубеждение” (“Pride and Prejudice,” gutenberg_id = 1342). Приведите тексты к tidy формату и уберите стопслова (английские стопслова есть в переменной stop_words пакета tidytext). Приведите, сколько получилось слов в романе “Чувство и чувствительность” после удаления стопслов: Приведите, сколько получилось слов в романе “Гордость и предубеждение” после удаления стопслов: 6.2 tf-idf tf-idf — важная мера, которая позволяет выделять важные для текста слова. \\[tf = \\frac{количество\\ употреблений\\ единицы\\ в\\ тексте}{количество\\ уникальных\\ единиц\\ в тексте}\\] \\[idf = log\\left(\\frac{количество\\ документов\\ в\\ корпусе}{количество\\ документов\\ с\\ исследуемой\\ единицей}\\right)\\] \\[TfIdf = tf \\times idf\\] library(tidytext) all_texts %&gt;% count(doc_id, lemma) %&gt;% bind_tf_idf(lemma, doc_id, n) %&gt;% arrange(tf_idf) %&gt;% group_by(doc_id) %&gt;% top_n(5) %&gt;% ungroup() %&gt;% mutate(lemma = reorder_within(lemma, tf_idf, doc_id)) %&gt;% ggplot(aes(tf_idf, lemma))+ geom_col()+ facet_wrap(~doc_id, scales = &quot;free&quot;)+ scale_y_reordered() Давайте попробуем посчитать всего Зощенко одним корпусом: all_texts %&gt;% mutate(doc_id = ifelse(doc_id != &quot;gordon_geometry&quot;, &quot;zoshenko&quot;, &quot;gordon_geometry&quot;)) %&gt;% count(doc_id, lemma) %&gt;% bind_tf_idf(lemma, doc_id, n) %&gt;% arrange(tf_idf) %&gt;% group_by(doc_id) %&gt;% top_n(20) %&gt;% ungroup() %&gt;% mutate(lemma = reorder_within(lemma, tf_idf, doc_id)) %&gt;% ggplot(aes(tf_idf, lemma))+ geom_col()+ facet_wrap(~doc_id, scales = &quot;free&quot;)+ scale_y_reordered() Используя созданную ранее переменную с текстами Джейн Остин без стопслов, выделите по 20 слов, важных для каждого романа. 6.3 Предиктивный ввод текста На прошлом занятии мы разобрались, что пакет tidytext позволяет делить не только на отдльные слова, но и смотреть на биграммы. Частотность биграмм можно использовать в подсказке слова, которую мы видим в наших телефонах: zo %&gt;% unnest_tokens(&quot;bigram&quot;, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, into = c(&quot;word_1&quot;, &quot;word_2&quot;)) %&gt;% count(word_1, word_2, sort = TRUE) -&gt; bigrams Теперь у нас есть биграмы: bigrams %&gt;% filter(word_1 == &quot;однажды&quot;) bigrams %&gt;% filter(word_1 == &quot;днем&quot;) bigrams %&gt;% filter(word_1 == &quot;присела&quot;) bigrams %&gt;% filter(word_1 == &quot;ждет&quot;) bigrams %&gt;% filter(word_1 == &quot;а&quot;) %&gt;% head() bigrams %&gt;% filter(word_1 == &quot;я&quot;) %&gt;% head() bigrams %&gt;% filter(word_1 == &quot;говорю&quot;) %&gt;% head() bigrams %&gt;% filter(word_1 == &quot;не&quot;) %&gt;% head() bigrams %&gt;% filter(word_1 == &quot;могу&quot;) %&gt;% head() Вот мы и получили предложение “Однажды днем присела ждет, а я говорю: ‘не могу’.” На большом корпусе результаты будут лучше, но легко предсатвить, как сделать из этого рабочую функцию. Можно переиначить задачу и работать с символами, тогда это будет ближе к T9 на современных телефонах. Используя тексты обоих романов создайте генератор текстов, основанный на биграммах. Какое трехсловное предложение получится, если выбирать самое частотную пару, и начать со слова I? I was there I am sure I should go 6.4 Анализ тональности Linis Crowd лемма значение среднеквадратичное отклонение РуСентиЛекс: слово или словосочетание, часть речи или синтаксический тип группы, слово или словосочетание в лемматизированной форме, тональность: позитивная (positive), негативная(negative), нейтральная (neutral) или неопределеная оценка, зависит от контекста (positive/negative), источник: оценка (opinion), чувство (feeling), факт (fact), если тональность отличается для разных значений многозначного слова, то перечисляются все значения слова по тезаурусу РуТез и дается отсылка на сооветствующее понятие - имя понятия в кавычках. Мы будем использовать датасет, составленный на базе Linis Crowd ru_sentiments &lt;- read_csv(&quot;https://raw.githubusercontent.com/agricolamz/2020_HSE_DPO/master/data/ru_sentiment_linis-crowd.csv&quot;) all_texts %&gt;% group_by(doc_id) %&gt;% left_join(ru_sentiments, by = c(&quot;lemma&quot; = &quot;words&quot;)) %&gt;% mutate(value = ifelse(is.na(value), 0, value)) %&gt;% group_by(doc_id, sentence_id) %&gt;% summarise(value = sum(value)) %&gt;% mutate(color = ifelse(value &gt;= 0, &quot;positive&quot;, &quot;negative&quot;)) %&gt;% ggplot(aes(sentence_id, value, fill = color))+ geom_col()+ facet_wrap(~doc_id, scales = &quot;free&quot;) 6.5 Тематическое моделирование LDA (Latent Dirichlet allocation) — один из базовых способов, используемый в тематическом моделировании. Основное идея алгаритма, заключается в том, что каждый текст может являтся смесью тем, а каждая тема имеет вероятность более высокую вероятность генерировать некоторые слова, и более низкую вероятность генерировать слова общие для всех тем. На вход подается посчитанный список слов для каждого текста. library(topicmodels) all_texts %&gt;% count(doc_id, lemma) %&gt;% cast_dtm(doc_id, lemma, n) %&gt;% # особая функция LDA(k = 2, # количество тем control = list(seed = 42) # повзоляет воспроизвести один и тот же анализ, можно убрать ) -&gt; lda Посмотрим, какие слова характерны для какой темы (т. е. самые частотные в теме): lda %&gt;% tidy(matrix = &quot;beta&quot;) %&gt;% # приводим модель в tidy формат group_by(topic) %&gt;% top_n(15, beta) %&gt;% ungroup() %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() Посмотрим, какие слова специфичны для каждой из тем (т. е. частота значительно больше в одной теме, чем в другой): lda %&gt;% tidy(matrix = &quot;beta&quot;) %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic1 &gt; .006 | topic2 &gt; .002) %&gt;% # эти значения нужно вручную подбирать mutate(log_ratio = log2(topic2 / topic1), term = fct_reorder(term, log_ratio)) %&gt;% ggplot(aes(log_ratio, term))+ geom_col() Посмотрим на распределение текстов по темам: lda %&gt;% tidy(matrix = &quot;gamma&quot;) "]]
